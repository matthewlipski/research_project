@inproceedings{10.1145/3412449.3412551,
    author = {Wang, Qing and Zuniga, Marco},
    title = {Passive Visible Light Networks: Taxonomy and Opportunities},
    year = {2020},
    isbn = {9781450380997},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3412449.3412551},
    doi = {10.1145/3412449.3412551},
    abstract = {Artificial lighting has been used mainly for illumination for more than a century. Only recently, we have started to transform our lighting infrastructure to provide new services such as sensing and communication. These advancements have two key requirements: the ability to modulate light sources (for data transmission) and the presence of photodetectors on objects (for data reception). These requirements assume that the system has direct control over the transmitter and receiver, as in any traditional communication system. But not all lights can be modulated, and most objects do not have photodetectors. To overcome these limitations, researchers are developing novel networks that (i) exploit passive light sources that cannot be directly modulated, such as the sun, and (ii) leverage reflections from the external surfaces of objects to create a new generation of sensing and communication networks with visible light that is sustainable and does not require active control over the system. In this survey, we propose a taxonomy to analyze state-of-the-art contributions. We also identify the overarching principles, challenges, and opportunities of this new rising area.},
    booktitle = {Proceedings of the Workshop on Light Up the IoT},
    pages = {42–47},
    numpages = {6},
    keywords = {opportunities, taxonomy, passive visible light communication, passive visible light sensing, applications},
    location = {London, United Kingdom},
    series = {LIOT '20}
}

@article{Alzubi_2018,
    doi = {10.1088/1742-6596/1142/1/012012},
    url = {https://doi.org/10.1088/1742-6596/1142/1/012012},
    year = 2018,
    month = {nov},
    publisher = {{IOP} Publishing},
    volume = {1142},
    pages = {012012},
    author = {Jafar Alzubi and Anand Nayyar and Akshi Kumar},
    title = {Machine Learning from Theory to Algorithms: An Overview},
    journal = {Journal of Physics: Conference Series},
}

@Inbook{Wang2003,
    author="Wang, Sun-Chong",
    title="Artificial Neural Network",
    bookTitle="Interdisciplinary Computing in Java Programming",
    year="2003",
    publisher="Springer US",
    address="Boston, MA",
    pages="81--100",
    abstract="Inspired by the sophisticated functionality of human brains where hundreds of billions of interconnected neurons process information in parallel, researchers have successfully tried demonstrating certain levels of intelligence on silicon. Examples include language translation and pattern recognition software. While simulation of human consciousness and emotion is still in the realm of science fiction, we, in this chapter, consider artificial neural networks as universal function approximators. Especially, we introduce neural networks which are suited for time series forecasts.",
    isbn="978-1-4615-0377-4",
    doi="10.1007/978-1-4615-0377-4_5",
    url="https://doi.org/10.1007/978-1-4615-0377-4_5"
}

@INPROCEEDINGS{8342164,
    author={Andrade, Liliana and Prost-Boucle, Adrien and Pétrot, Frédéric},
    booktitle={2018 Design, Automation \& Test in Europe Conference \& Exhibition (DATE)},
    title={Overview of the state of the art in embedded machine learning},
    year={2018},
    volume={},
    number={},
    pages={1033-1038},
    doi={10.23919/DATE.2018.8342164}
}

@inproceedings{MLSYS2021_d2ddea18,
    author = {David, Robert and Duke, Jared and Jain, Advait and Janapa Reddi, Vijay and Jeffries, Nat and Li, Jian and Kreeger, Nick and Nappier, Ian and Natraj, Meghna and Wang, Tiezhen and Warden, Pete and Rhodes, Rocky},
    booktitle = {Proceedings of Machine Learning and Systems},
    editor = {A. Smola and A. Dimakis and I. Stoica},
    pages = {800--811},
    title = {TensorFlow Lite Micro: Embedded Machine Learning for TinyML Systems},
    url = {https://proceedings.mlsys.org/paper/2021/file/d2ddea18f00665ce8623e36bd4e3c7c5-Paper.pdf},
    volume = {3},
    year = {2021}
}

@INPROCEEDINGS{4912759,
    author={Liu, Jiayang and Wang, Zhen and Zhong, Lin and Wickramasuriya, Jehan and Vasudevan, Venu},
    booktitle={2009 IEEE International Conference on Pervasive Computing and Communications},
    title={uWave: Accelerometer-based personalized gesture recognition and its applications},
    year={2009},
    volume={},
    number={},
    pages={1-9},
    doi={10.1109/PERCOM.2009.4912759}
}

@article{article,
    author = {Lahamy, Herve and Lichti, Derek},
    year = {2012},
    month = {05},
    pages = {},
    title = {REAL-TIME HAND GESTURE RECOGNITION USING RANGE CAMERAS}
}

@ARTICLE{8947919,
    author={Duan, Haihan and Huang, Miao and Yang, Yanbing and Hao, Jie and Chen, Liangyin},
    journal={IEEE Access},
    title={Ambient Light Based Hand Gesture Recognition Enabled by Recurrent Neural Network},
    year={2020},
    volume={8},
    number={},
    pages={7303-7312},
    doi={10.1109/ACCESS.2019.2963440}
}

@article{HUSKEN2003223,
    title = {Recurrent neural networks for time series classification},
    journal = {Neurocomputing},
    volume = {50},
    pages = {223-235},
    year = {2003},
    issn = {0925-2312},
    doi = {https://doi.org/10.1016/S0925-2312(01)00706-8},
    url = {https://www.sciencedirect.com/science/article/pii/S0925231201007068},
    author = {Michael Hüsken and Peter Stagge},
    keywords = {Recurrent neural network, Classification, Time series, Multitask learning, Generalization},
    abstract = {Recurrent neural networks (RNN) are a widely used tool for the prediction of time series. In this paper we use the dynamic behaviour of the RNN to categorize input sequences into different specified classes. These two tasks do not seem to have much in common. However, the prediction task strongly supports the development of a suitable internal structure, representing the main features of the input sequence, to solve the classification problem. Therefore, the speed and success of the training as well as the generalization ability of the trained RNN are significantly improved. The trained RNN provides good classification performance and enables the user to assess efficiently the degree of reliability of the classification result.}
}

@article{DBLP:journals/corr/abs-1801-06105,
    author    = {Yuhuang Hu and
               Adrian E. G. Huber and
               Jithendar Anumula and
               Shih{-}Chii Liu},
    title     = {Overcoming the vanishing gradient problem in plain recurrent networks},
    journal   = {CoRR},
    volume    = {abs/1801.06105},
    year      = {2018},
    url       = {http://arxiv.org/abs/1801.06105},
    eprinttype = {arXiv},
    eprint    = {1801.06105},
    timestamp = {Tue, 18 Sep 2018 12:35:49 +0200},
    biburl    = {https://dblp.org/rec/journals/corr/abs-1801-06105.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org}
}
@misc{https://doi.org/10.48550/arxiv.1506.04214,
    doi = {10.48550/ARXIV.1506.04214},
    url = {https://arxiv.org/abs/1506.04214},
    author = {Shi, Xingjian and Chen, Zhourong and Wang, Hao and Yeung, Dit-Yan and Wong, Wai-kin and Woo, Wang-chun},
    keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting},
    publisher = {arXiv},
    year = {2015},
    copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{yuksel-etal-2019-turkish,
    title = "Turkish Tweet Classification with Transformer Encoder",
    author = {Y{\"u}ksel, At{\i}f Emre  and
      T{\"u}rkmen, Ya{\c{s}}ar Alim  and
      {\"O}zg{\"u}r, Arzucan  and
      Alt{\i}nel, Berna},
    booktitle = "Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019)",
    month = sep,
    year = "2019",
    address = "Varna, Bulgaria",
    publisher = "INCOMA Ltd.",
    url = "https://aclanthology.org/R19-1158",
    doi = "10.26615/978-954-452-056-4_158",
    pages = "1380--1387",
    abstract = "Short-text classification is a challenging task, due to the sparsity and high dimensionality of the feature space. In this study, we aim to analyze and classify Turkish tweets based on their topics. Social media jargon and the agglutinative structure of the Turkish language makes this classification task even harder. As far as we know, this is the first study that uses a Transformer Encoder for short text classification in Turkish. The model is trained in a weakly supervised manner, where the training data set has been labeled automatically. Our results on the test set, which has been manually labeled, show that performing morphological analysis improves the classification performance of the traditional machine learning algorithms Random Forest, Naive Bayes, and Support Vector Machines. Still, the proposed approach achieves an F-score of 89.3 {\%} outperforming those algorithms by at least 5 points.",
}

@article{JMLR:v15:srivastava14a,
    author  = {Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
    title   = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
    journal = {Journal of Machine Learning Research},
    year    = {2014},
    volume  = {15},
    number  = {56},
    pages   = {1929--1958},
    url     = {http://jmlr.org/papers/v15/srivastava14a.html}
}

@inbook{inbook,
    author = {Berrar, Daniel},
    year = {2018},
    month = {01},
    pages = {},
    title = {Cross-Validation},
    isbn = {9780128096338},
    doi = {10.1016/B978-0-12-809633-8.20349-X}
}

@book{warden2020tinyml,
    title={TinyML: Machine Learning with TensorFlow Lite on Arduino and Ultra-low-power Microcontrollers},
    author={Warden, P. and Situnayake, D.},
    isbn={9781492052043},
    lccn={2020277178},
    url={https://books.google.nl/books?id=sB3mxQEACAAJ},
    year={2020},
    publisher={O'Reilly}
}

@misc{https://doi.org/10.48550/arxiv.1706.03762,
    doi = {10.48550/ARXIV.1706.03762},
    url = {https://arxiv.org/abs/1706.03762},
    author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
    keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {Attention Is All You Need},
    publisher = {arXiv},
    year = {2017},
    copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{KIM201972,
    title = {Predicting residential energy consumption using CNN-LSTM neural networks},
    journal = {Energy},
    volume = {182},
    pages = {72-81},
    year = {2019},
    issn = {0360-5442},
    doi = {https://doi.org/10.1016/j.energy.2019.05.230},
    url = {https://www.sciencedirect.com/science/article/pii/S0360544219311223},
    author = {Tae-Young Kim and Sung-Bae Cho},
    keywords = {Electric energy consumption, Deep learning, Convolutional neural network, Long short-term memory},
    abstract = {The rapid increase in human population and development in technology have sharply raised power consumption in today's world. Since electricity is consumed simultaneously as it is generated at the power plant, it is important to accurately predict the energy consumption in advance for stable power supply. In this paper, we propose a CNN-LSTM neural network that can extract spatial and temporal features to effectively predict the housing energy consumption. Experiments have shown that the CNN-LSTM neural network, which combines convolutional neural network (CNN) and long short-term memory (LSTM), can extract complex features of energy consumption. The CNN layer can extract the features between several variables affecting energy consumption, and the LSTM layer is appropriate for modeling temporal information of irregular trends in time series components. The proposed CNN-LSTM method achieves almost perfect prediction performance for electric energy consumption that was previously difficult to predict. Also, it records the smallest value of root mean square error compared to the conventional forecasting methods for the dataset on individual household power consumption. The empirical analysis of the variables confirms what affects to forecast the power consumption most.}
}
