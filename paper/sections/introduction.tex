\setlength{\parskip}{\baselineskip}%
\setlength{\parindent}{0pt}%

\section{Introduction}\label{sec:introduction}
\subsection{Research Overview}\label{subsec:research-overview}
Traditionally, physical buttons have been by far the most common way for users to interact with electronic devices in public settings, whether these are coffee machines, elevator panels, or train ticket machines.
However, the concern of disease transmission has become increasingly prevalent in recent years due to the COVID-19 pandemic, making it enticing to develop an alternative solution which does not require touch.
One such solution is the use of hand gestures to interact with public devices instead.
By performing hand motions such as swiping and tapping, users can effectively and intuitively interact with electronic devices with minimum risk of disease transmission.

However, there are a few key challenges to this approach which stand in the way of it replacing physical buttons in real-world applications.
\begin{enumerate}
    \item Additional hardware is needed to detect the positions of a user's hand while performing a gesture.
    The data output by this hardware is likely to be low in resolution since system costs should be minimized.
    \item Additional software must be implemented to recognize gestures based on hand position over time.
    While buttons are just simple digital inputs, one gesture can also be performed differently between different users, yet the system must be able to accurately classify it regardless.
    \item Gesture recognition must be done in real-time.
    Since the process of classifying gestures can be quite complex, the latency introduced by this may be significant.
    However, the user should not perceive any lag while using the system for a positive experience.
\end{enumerate}

\subsection{Research Question}\label{subsec:research-question}
Given the challenges in developing a neural network for gesture recognition, the goal of this paper can be summarized with the following research question:

\textbf{'Which neural network architecture is most appropriate for recognizing hand gestures on an Arduino Nano 33 BLE, using 3D-formatted data from OPT101 photodiodes?'}

This can then be segmented into the following sub-questions:
\begin{enumerate}
    \item Which neural network architectures produce the highest accuracy for hand gesture recognition?
    \item Which neural network architectures produce the lowest latency for hand gesture recognition?
    \item What is the minimum acceptable accuracy for recognizing hand gestures on an Arduino Nano 33 BLE?
    \item What is the maximum acceptable latency for recognizing hand gestures on an Arduino Nano 33 BLE?
    \item How can 3D-formatting data be exploited for better gesture recognition performance?
\end{enumerate}

\subsection{Contributions}\label{subsec:contributions}
This research overcomes these challenges by using data from OPT101 photodiodes, which is fed into a neural network to recognize gestures with high accuracy and low latency on an Arduino Nano 33 BLE microcontroller.
It is also part of a larger project which integrates this neural network into a full gesture recognition system, which is elaborated on in section 3.2.

Similar research which involves using photodiodes and machine learning to recognize hand gestures has already been conducted, but this paper improves on existing solutions in a number of ways:
\begin{enumerate}
    \item A high level of accuracy is maintained with fewer photodiodes, and therefore fewer model input features, than existing solutions.
    \item The data from photodiodes is 3D-formatted, which better preserves temporal information and improves recognition accuracy.
    An explanation for what 3D-formatting involves can be found in section~\ref{subsec:3d-formatted-data}.
    \item The neural network used to classify hand gestures is shallower than competing solutions, leading to reduced system latency while maintaining a high level of accuracy.
\end{enumerate}

A discussion regarding existing research in this field is found in section~\ref{sec:related-work}, which provides more context to these improvements.

%The planned use case for this technology is hands-free navigation of menus which has especially gained relevance due to the restrictions imposed during the COVID-19 pandemic, which has showed that gestures can be an appealing alternative to physical buttons in social settings.
%The most important existing literature for this research comes from Pete Warden and Daniel Situnayake, who are pioneers in the field of embedded AI and authors of the book "TinyML: Machine Learning with TensorFlow Lite on Arduino and Ultra-Low-Power Microcontrollers"\cite{warden2020tinyml}, as well as Qing Wang and Marco Zuniga, who have laid the groundwork for embedded AI specifically in the context of hand gesture recognition\cite{10.1145/3412449.3412551}.
%The research conducted for this paper expands on existing work in this exact way, by using a lower power microcontroller and significantly fewer photo diodes than in the solution created by Wang and Zuniga, while maintaining or improving the classification accuracy of hand gestures.
